<!doctype html>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/all.min.css">
<link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Open+Sans&display=swap">
<html lang="en">
<head>
  <meta charset="utf-8">
  <title>Flow to the Mode: Mode-Seeking Diffusion Autoencoders
    for State-of-the-Art Image Tokenization</title>
  <link rel="stylesheet" type="text/css" href="flowmo_gallery_styles.css">
  <script src="flowmo_comparison.js"></script>
  <style>
    /* Elegant Mathematical Blog Style with Light Blue/Gray Tones */
    body {
      margin: 0 auto;
      padding: 40px 20px;
      max-width: 1200px;
      font-size: 1.3em;
      font-family: "Open Sans", sans-serif;
      background-color: #ffffff;
      color: #333;
      line-height: 1.6;
    }
    .button {
        display: inline-block;
        padding: 2px 10px;
        border: none;
        cursor: pointer;
        text-decoration: none;
    }
    .is-dark {
        background-color: #333;
        color: white;
    }
    .is-rounded {
        border-radius: 30px;
    }
    .icon {
        margin-right: 5px;
    }
    a {
      text-decoration: none;
      color: #3a6ea5;
    }
    a:hover {
      /* text-decoration: none; */
      /* color: #2d4f8f; */
    }
    a {
      /* text-decoration: none; */
      color: #22b;
    }
    a:hover {
      /* text-decoration: none; */
      /* color: #22b; */
    }
    a:hover > span.icon{
      text-decoration: none
    }
    a:hover img {
      opacity: .5;
    }
    /* h1, h2 {
      font-family: "Open Sans", sans-serif;
      font-weight: 300;
      color: #222;
    }
    h1 {
      text-align: center;
      margin-bottom: 0px;
      font-size: 2.4em;
    }
    h2 {
      text-align: center;
      margin: 0 0 0px 0;
      font-size: 2.0em;
      font-style: italic;
      color: #555;
    } */
    .authors {
      text-align: center;
      font-size: 1em;
      color: #666;
      margin-bottom: 40px;
    }
    body {
        padding: 40px 20px;
        margin: 0 auto;
        max-width: 1200px; /* Adjust as needed */
    }

    h1, h2, h3, h4, h5, h6 {
        margin-left: 0;
        padding-left: 0;
    }

    /* To ensure consistency, explicitly reset the margins of body paragraphs too */
    p {
        margin-left: 0;
        padding-left: 0;
    }
    h1 {
        font-size: 2.25em;
        text-align: center;
    }
    h3 {
        font-size: 2.25em;
        text-align: center;
        border-bottom: 2px solid #ccc;
    }
    h4 {
        font-size: 1.25em;
        text-align: left;
        border-bottom: 1px solid #ccc;
    }
    /* h3 {
      margin-top: 50px;
      border-bottom: 1px solid #ccc;
      padding-bottom: 10px;
      font-weight: 300;
      color: #222;
    }
    p {
      max-width: 1100px;
      margin: 0px auto;
      font-size: 1.1em;
    } */
    pre, code {
      background: #e8f0fe;
      /* padding: 10px; */
      border-radius: 5px;
      overflow-x: auto;
      font-family: "Open Sans", sans-serif;
    }
    .clearfix::after {
      content: "";
      display: table;
      clear: both;
    }
    /* Math styling */
    .math-block {
      text-align: center;
      margin: 20px 0;
      font-size: 1.1em;
    }
  </style>
  <!-- Include MathJax -->
  <script>
    window.MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']]
      }
    };
  </script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>

  <h1>Flow to the Mode: Mode-Seeking Diffusion Autoencoders
    for State-of-the-Art Image Tokenization</h1>
  <h2></h2>
  <p class="authors">
    <a href="https://kylesargent.github.io">Kyle Sargent<sup>1</sup></a>,
    <a href="https://www.kylehsu.org">Kyle Hsu<sup>1</sup></a>,
    <a href="https://web.eecs.umich.edu/~justincj">Justin Johnson<sup>2</sup></a>,
    <a href="https://profiles.stanford.edu/fei-fei-li">Li Fei-Fei<sup>1</sup></a>, 
    <a href="https://jiajunwu.com">Jiajun Wu<sup>1</sup></a>,
    <br>
    Stanford University<sup>1</sup>, University of Michigan<sup>2</sup>
  </p>

  <h2 align="center" style="font-size: 24px;">
    <span class="link-block">
      <a href="https://www.arxiv.org/abs/2503.11056"
         class="external-link button is-normal is-rounded is-dark">
        <span class="icon">
          <i class="fas fa-file-pdf"></i>
        </span>
        <span>Paper</span>
      </a>
    </span>
    <span class="link-block">
      <a href="https://github.com/kylesargent/FlowMo"
         class="external-link button is-normal is-rounded is-dark">
        <span class="icon">
          <i class="fab fa-github"></i>
        </span>
        <span>Code</span>
      </a>
    </span>
  </h2>
 

    <!doctype html>
    <html lang="en">
    <head>
    <meta charset="utf-8">
    <title>Gallery Links</title>
    <style>
        .link-container {
        display: flex;
        justify-content: center;
        gap: 20px; /* space between the links */
        margin-top: 40px;
        }
        .link-container a {
        text-decoration: none;
        font-size: 1.2em;
        color: #0066cc;
        }
        .link-container a:hover {
        text-decoration: underline;
        }
    </style>

    <h3></h3>
    <p>
        Please check out our galleries
        (<a href="https://kylesargent.github.io/flowmo_lo_gallery.html">FlowMo-Lo gallery</a>,
        <a href="https://kylesargent.github.io/flowmo_hi_gallery.html">FlowMo-Hi gallery</a>)
        for more visual comparisons against baselines!
    
      </p>
    <div id="comparisonContainer" class="comparison-container"></div>
    <script>
      // Example usage: Replace these paths with your actual image URLs
      const container = document.getElementById('comparisonContainer');
      // Create 9 comparison blocks (or as many as desired)
    //   for (let i = 0; i < 2; i++) {
        
        idx = 0;
        half = 'top_bottom'; 
        const original = `images/flowmo/flowmo_a_gallery/col${idx}_gt_${half}.png`;
        const method1  = `images/flowmo/flowmo_a_gallery/col${idx}_magvit_${half}.png`;
        const method2  = `images/flowmo/flowmo_a_gallery/col${idx}_ours_${half}.png`;
        const block = createImageComparison(original, method1, method2, "OpenMagViT-V2", "FlowMo-Lo");
        container.appendChild(block);

        idx = 1;
        half = 'bottom_top'; 
        const originalb = `images/flowmo/flowmo_b_gallery/col${idx}_gt_${half}.png`;
        const method1b  = `images/flowmo/flowmo_b_gallery/col${idx}_llamagen_${half}.png`;
        const method2b  = `images/flowmo/flowmo_b_gallery/col${idx}_ours_${half}.png`;
        const blockb = createImageComparison(originalb, method1b, method2b, "LlamaGen-32", "FlowMo-Hi");
        container.appendChild(blockb);
    </script>

    <!-- </head>
    <body>
    Check out our galleries for extended visual comparisons!
    <div class="link-container">
        <a href="https://kylesargent.github.io/flowmo_lo_gallery.html">Gallery A - comparison with OpenMagViT-V2</a>
        <a href="https://kylesargent.github.io/flowmo_hi_gallery.html">Gallery B - comparison with LlamaGen-32</a>
    </div>
    </body>
    </html> -->

    <h3>Introduction</h3>
    <p>
      Since the advent of popular visual generation frameworks like VQGAN and Latent Diffusion Models, state-of-the-art image generation systems have generally been two-stage systems that first tokenize or compress visual data into
  a lower-dimensional latent space before learning a generative model. Tokenizer training typically follows a standard
  recipe in which images are compressed and reconstructed
  subject to a combination of MSE, perceptual, and adversarial losses. Diffusion autoencoders have been proposed
  in prior work as a way to learn end-to-end perceptually-oriented image compression, but have not yet shown state-of-the-art performance on the competitive task of ImageNet1K reconstruction. In this work, we propose FlowMo, a
  transformer-based diffusion autoencoder. FlowMo achieves a new state-of-the-art for image tokenization at multiple bitrates. We achieve this without using convolutions, adversarial losses, spatially-aligned 2-dimensional latent codes,
  or distilling from other tokenizers. Our key insight is that
  FlowMo training should be broken into a mode-matching
  pre-training stage and a mode-seeking post-training stage.
  We conduct extensive analysis and ablations, and we additionally train generative models atop the FlowMo tokenizer
  and verify the performance.
    </p>
    <figure style="text-align: left;">
        <img src="images/flowmo/teaser (24).png" alt="Vector Graphic" width="600" style="display: block; margin: 0 auto;">
        <figcaption>Figure 1: 
            When trained for reconstruction at a low bitrate
             (FlowMo-Lo) or high bitrate (FlowMo-Hi), FlowMo achieves
            state-of-the art image tokenization performance. Moreover, FlowMo is a transformer-based
            diffusion autoencoder which does not use convolutions, adversarial losses, or proxy objectives from auxiliary tokenizers.
        </figcaption>
      </figure>

  <h3>Method</h3>
    We'll briefly outline the architecture and training stages below. For an in-depth summary, check out our paper!

  <h4>Architecture</h4>
  <p>
    FlowMo is implemented as a diffusion autoencoder with transformer-based encoder and decoder. The architecture diagram is shown below.
  </p>

  <img src="images/flowmo/arch_v2.png" alt="Vector Graphic", width=1200>

  <!-- <h4>Encoding and Quantization</h4> -->
  <p>
    The encoder maps an image <em>x</em> (with an initial latent <em>câ‚€</em>) to a continuous latent representation:
  </p>
  <div class="math-block">
    $$\hat{c} = e_{\theta}(x, c_0)~,$$
  </div>
  <p>
    which is quantized using lookup-free quantization, as
  </p>
  <div class="math-block">
    $$c = q(\hat{c}) = 2 \cdot \mathbf{1}[\hat{c} \geq 0] - 1~.$$
  </div>

  <!-- <h4>Decoding via Diffusion</h4> -->
  <p>
    The decoder models a velocity field that transforms a noisy image back to a clean one. A noisy image <em>x<sub>t</sub></em> is defined as:
  </p>
  <div class="math-block">
    $$x_t = t \cdot z + (1 - t) \cdot x \quad \text{with} \quad z \sim \mathcal{N}(0, I), \quad t \in [0, 1] ~ .$$
  </div>
  <p>
    The decoder then predicts:
  </p>
  <div class="math-block">
    $$v = d_{\theta}(x_t, c, t)~.$$
  </div>

  <h4>Mode-matching pre-training</h4>
  <p>
    First, FlowMo is trained end-to-end as a diffusion autoencoder. A diagram of the Stage 1A training is shown below:
    <img src="images/flowmo/stage_1a.png" alt="Vector Graphic", width=600, style="display: block; margin: 0 auto;">
  </p>
  <p>The model is trained end-to-end with a diffusion loss</p>
  <div class="math-block">
    $$\mathcal{L}_{\text{flow}} = \mathbb{E}\Big[\big\|x - z - d_{\theta}(x_t, q(e_{\theta}(x)), t)\big\|_2^2\Big]~.$$
  </div>
  <p>
    Additional losses include a perceptual loss \( \mathcal{L}_{\text{perc}} \) and quantization losses \( \mathcal{L}_{\text{commit}} \) and \( \mathcal{L}_{\text{ent}} \). The overall loss is:
  </p>
  <div class="math-block">
    $$\mathcal{L}_{\text{flow}} + \lambda_{\text{perc}} \mathcal{L}_{\text{perc}} + \lambda_{\text{commit}} \mathcal{L}_{\text{commit}} + \lambda_{\text{ent}} \mathcal{L}_{\text{ent}}~.$$
  </div>

  <h4>Mode-seeking post-training</h4>
  <p>
    In Stage 1B of training, the goal is to constrain the model to drop modes of the reconstruction distribution which are not perceptually close to the original image.
    A diagram is shown below:
    </p><p>
    <img src="images/flowmo/stage_1b.png" alt="Vector Graphic", width=650, style="display: block; margin: 0 auto;">
    
</p><p>In this stage, the encoder is fixed and the decoder is fine-tuned using a sample-level perceptual loss:
  </p>
  <div class="math-block">
    $$\mathcal{L}_{\text{sample}} = \mathbb{E}\Big[d_{\text{perc}}\Big(x, \Big(d_{t_n} \circ d_{t_{n-1}} \circ \cdots \circ d_{t_1}\Big)(x)\Big]\Big)~.$$
  </div>
  <p>
    The total loss during this stage becomes:
  </p>
  <div class="math-block">
    $$\mathcal{L}_{\text{flow}} + \lambda_{\text{sample}} \mathcal{L}_{\text{sample}}~.$$
  </div>

  <h3>Conclusion</h3>
  <p>
    Please check out our paper for detailed comparisons and analysis, and our galleries 
    <a href="https://kylesargent.github.io/flowmo_lo_gallery.html">(FlowMo-Lo</a>
    <a href="https://kylesargent.github.io/flowmo_hi_gallery.html">, FlowMo-Hi)</a>
    for interactive comparisons.

  </p>

  <h3>Citation</h3>
  <pre><code>@misc{sargent2025flowmodemodeseekingdiffusion,
      title={Flow to the Mode: Mode-Seeking Diffusion Autoencoders for State-of-the-Art Image Tokenization}, 
      author={Kyle Sargent and Kyle Hsu and Justin Johnson and Li Fei-Fei and Jiajun Wu},
      year={2025},
      eprint={2503.11056},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2503.11056}, 
}</code></pre>

</body>
</html>