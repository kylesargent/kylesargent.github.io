<!doctype html>
<meta charset=utf-8>
<title>VQ3D: Learning a 3D-Aware Generative Model on ImageNet</title>
<style>
body {
  margin: 0 auto;
  padding: 30px 20px 50px;
  font-family: sans-serif;
  background: #ffffff;
  color: #222;
  max-width: 900px;
}
p {
  line-height: 150%;
  max-width: 45em;
}
h1, h2 {
  font-weight: 200;
}
h2 a {
  color: #000;
}

h3 {
  margin-top: 40px;
}
a {
  text-decoration: none;
  color: #22b;
}
a:hover {
  text-decoration: underline;
  color: #22b;
}
a:hover img {
  opacity: .5;
}
.paper img { background: #888; }
.video img {
  border-radius: 6px;
  background: #000;
}
h1 span {
  font-size: 90%;
  line-height: 1.5em;
}
h1 span::before { content: "("; }
h1 span::after { content: ")"; }
h1 span::before, h1 span::after {
  vertical-align: .06em;
}
h2 span {
  line-height: 2em;
}
em {
  color: #333;
  font-weight: bold;
  font-style: normal;
}
.teaser {
  max-width: 45em;
  margin: 40px 0 60px 0;
  white-space: nowrap;
  position: relative;
}
.teaser .imgs .full {
  width: 100%;
}
.teaser .imgs .center {
  position: absolute;
  width: 32%;
  top: -3%;
  left: 34%;
}
.teaser .labels span {
  display: inline-block;
  width: 33%;
  text-align: center;
  padding-top: 10px;
}
.imgContainer{
    float:left;
}
.bibtex {
  white-space: pre;
  font-family: monospace;
  line-height: 150%;
  background: #fff;
  padding: 10px;
  display: inline-block;
  border-radius: 4px;
}
.crow > * {
  vertical-align: middle;
  margin-right: 20px;
}
a.img {
  display: inline-block;
}
.links {
  display: inline-block;
  line-height: 150%;
  padding: 8px 0;
}
.links a {
  padding: 0 3px;
}
.examples {
  padding: 10px 0;
  max-width: 50em;
}
.examples img {
  display: inline-block;
  width: 192px;
  height: 108px;
}

</style>
<h1>VQ3D: Learning a 3D-Aware Generative Model on ImageNet<br>

<!-- <img src="data:image/png;base64,"> -->

<h3>Introduction</h3>
<p>VQ3D is a 2-stage autoencoder based on ViT-VQGAN. We use a novel 3D-aware NeRF-based decoder as well as depth losses and adversarial supervision on main and novel views to encourage learning of 3D-aware representations.

    The resulting model has multiple capabilities. Stage 1 of our model is an encoder-decoder architecture which can encode unseen, unposed RGB images and produce a NeRF with consistent depth and high-quality novel views. Stage 2 is a fully generative model
    of 3D-aware images. For technical details, please consult our paper. Thanks! 
</p>
&nbsp

<h3>Stage 2: Example generated 3D images:</h3>
We show generated 3D images as well as normalized disparity below. Our method is capable of training on and then generating examples from the full ImageNet dataset of 1.2 million training images and 1000 classes.<br>
&nbsp
<img width="900" height="300" src="videos/vq3d/fully_generated.gif">

<h3>Stage 1: Example single-view 3D reconstructions with novel views:</h3>
We show results from manipulating real images to generate novel views. Our model achieves good reconstruction at the canonical viewpoint and plausible novel views.<br>
&nbsp
<img width="900" height="300" src="videos/vq3d/novel_views_webpage_3.gif">
&nbsp

<h3>Example object manipulations within a scene:</h3>
Our Stage 1 can convert a single RGB input image into a NeRF, and then render novel views while manipulating adding objects. The only input to the model is a single unposed RGB image which is unseen during training. The NeRF is created without any optimization in a single forward pass of our network. Still, the reconstructed scene has plausible geometry, occlusion, and disoccluded pixels.<br>
&nbsp
<div class="image123">
    <div class="imgContainer">
        <img width="450" height="450" src="videos/vq3d/manip_orig.png">
        <p align="center">Original Image</p>
    </div>
    <div class="imgContainer">
        <img width="450" height="450" src="videos/vq3d/manip.gif">
        <p align="center">Rendered novel views with additional object</p>
    </div>
</div>
&nbsp

<h3>Example generated scenes from CompCars:</h3>
Our model also achieves good performance on CompCars.<br>
&nbsp
<img width="900" height="450" src="videos/vq3d/compcars.gif">
&nbsp

<h3>Example interpolation between two images:</h3>
Our Stage 1 can convert compute two NeRFs from two RGB images, and then interpolate between them. Vector-quantized models do not guarantee high quality interpolation, and achieving high-quality semantically aware interpolation is a direction for future work.<br>
&nbsp
<div class="image123">
    <div class="imgContainer">
        <img width="225" height="225" src="videos/vq3d/interp_start_orig.png">
        <p align="center">Input start image</p>
    </div>
    <div class="imgContainer">
        <img width="225" height="225" src="videos/vq3d/interp_end_orig.png">
        <p align="center">Input end image</p>
    </div>
    <div class="imgContainer">
        <img width="450" height="225" src="videos/vq3d/interp.gif">
        <p align="center">Interpolation</p>
    </div>
</div>
&nbsp


<h3>Importance of pointwise depth loss</h3>
Our depth loss constraints the NeRF disparity to lie close to the pseudo-GT disparity in a pointwise fashion. It supervises the pointwise volumetric rendering weights rather than the accumulated disparity. Below we see the affect of training with a depth loss on the accumulated disparity (left) versus our depth loss on the pointwise volumetric rendering weights (right). <br>
&nbsp
<div class="image123">
    <div class="imgContainer">
        <img width="450" height="225" src="videos/vq3d/depth_abl_accum.gif">
        <p align="center">Trained with depth loss on accumulated depth</p>
    </div>
    <div class="imgContainer">
        <img width="450" height="225" src="videos/vq3d/depth_abl_pointwise.gif">
        <p align="center">Trained with our pointwise depth loss</p>
    </div>
</div>
&nbsp